@workspace Scan the entire project structure, tech stack, coding conventions, and error-handling patterns before generating anything. All new files must follow the existing directory layout, naming conventions, and linting rules.

Implement a complete speech analysis module for the interview system with the following four parts:

**Part 1 — Audio Capture & Confidence Tracking (Client Side)**
Create an `AudioRecorder` component using `navigator.mediaDevices.getUserMedia` and the `MediaRecorder` API to capture microphone audio. Use the Web Audio API `AnalyserNode` to sample volume (RMS) in real time via `requestAnimationFrame`. Maintain a reactive `confidenceScore` (0–100): if the rolling average RMS stays above a configurable `CONFIDENCE_THRESHOLD` for more than 70% of the recording duration, label it "High Confidence." On stop, POST the recorded Blob as `multipart/form-data` to `POST /api/speech/analyze`. Include full TypeScript types, loading and error states, and cleanup of all audio streams on unmount.

**Part 2 — Transcription & Linguistic Metrics (Server Side)**
Add a `POST /api/speech/analyze` route following the existing router and controller pattern. Accept the uploaded audio file using whatever upload middleware the project already uses (e.g., multer). Send the file to a transcription service (check #file:package.json for an existing SDK such as AssemblyAI or Deepgram; if none exists, default to AssemblyAI) and request word-level timestamps. From the transcription response compute: (a) WPM as `(totalWords / audioDurationInSeconds) * 60`, (b) Filler Word Count by matching each word case-insensitively against `["um","uh","like","basically","actually","you know","sort of","kind of","I mean"]` and collecting their timestamps, (c) Hesitation Pauses by iterating sequential word timestamps and counting every gap greater than 1.5 seconds between one word's end and the next word's start along with the timestamp positions. Return the response as: `{ transcript: string, durationSeconds: number, wpm: number, fillerCount: number, fillers: { word: string, timestamp: number }[], hesitationCount: number, hesitations: { startSec: number, endSec: number }[] }`. Add input validation, file-size limits, and try-catch error handling consistent with the existing API error responses.

**Part 3 — AI Coaching Feedback (Server Side)**
Create a service function `generateCoachingFeedback(transcript, metrics)` that calls the LLM client already configured in the project (check #file:package.json; default to OpenAI if none exists). Use a system prompt: "You are a senior career coach specializing in interview communication skills. You will receive a candidate's spoken transcript and speech metrics. Return ONLY valid JSON matching the provided schema — no markdown, no extra keys." Request the response in this exact shape: `{ clarityScore: number (0-100), strengths: string[], weaknesses: string[], actionableSteps: [string, string, string] }`. Parse and validate the LLM JSON; if parsing fails, retry once, then return a graceful error. Wire this into the `/api/speech/analyze` route so the final response merges the metrics from Part 2 and the coaching feedback from Part 3 into a single JSON payload.

**Part 4 — Analytics Dashboard (Client Side)**
Create a `SpeechAnalytics` page or component. Render the Confidence Score and Clarity Score as circular or linear progress bars using the UI library already in the project. Display WPM, Filler Count, and Hesitation Count in a card-style stats grid. Show strengths, weaknesses, and actionable steps in clearly separated lists with appropriate color coding (green for strengths, red for weaknesses, blue for action steps). Include a scrollable transcript panel that highlights filler words inline with a colored badge or tag. Make the entire view responsive and consistent with the existing design system.

**Requirements across all parts:** Place every new file in the correct existing directory (components, routes, services, utils, types). Create shared TypeScript interfaces for all API request and response shapes. Add any required API key environment variables to `.env.example` with placeholder values. Use try-catch on the server and error boundaries or toast notifications on the client. Never hardcode secrets — read from `process.env`. Add brief JSDoc comments on all exported functions.